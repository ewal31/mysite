<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Implementing Kernel Density Estimation - Univariate</title>
        <link rel="stylesheet" href="../css/syntax.css" />
        <link rel="stylesheet" type="text/css" href="../css/default.css" />
    </head>
    <body>
        <div class="backgroundpattern"></div>
        <div id="body">
            <div id="header">
                <div id="logo">
                    <a href="../">Ed's Site</a>
                </div>
                <div id="navigation">
                    <a href="../">Home</a>
                    <!-- <a href="/about.html">About</a> -->
                    <!-- <a href="/contact.html">Contact</a> -->
                    <a href="../blog.html">Blog</a>
                    <a href="../notes.html">Notes</a>
                    <a href="http://github.com/ewal31/mysite">Source</a>
                </div>
            </div>

            <div id="content">
                <h1>Implementing Kernel Density Estimation - Univariate</h1>
                <div class="info">
    Posted on June  2, 2023
    
</div>

<p><img src="../img/2023/Hist2Kernel.gif" id="imgexpandtoborder" class="img" alt="distribution.gif" /></p>
<h3>Summary</h3>
<p>Kernel Density Estimation approximates a probability density function from collected samples. In contrast to a histogram, we do not need first to discretise the data by placing it into bins, instead building a continuous estimate. Applied to height measurements of adults, for example, we obtain a continuous density describing the likelihood of each possible height.</p>
<p>This article will first introduce the concept of a Kernel as applied to smoothing data points before implementing it in the <a href="https://julialang.org/">Julia</a> programming language. Next, this implementation will be modified to calculate the Kernel Density Estimate of 1-dimensional data before exploring possibilities for determining an optimal bandwidth.</p>
<h3>Kernel Smoothing</h3>
<p>A Kernel function acts as a weighted measure of distance between objects. It is not restricted exclusively to points in a space - see, for example, the <a href="https://en.wikipedia.org/wiki/String_kernel">string kernel</a> - it should, however, in most cases be both <em>symmetric</em> and <em>non-negative</em>. For example, a Kernel function operating on real numbers generally has the following form:</p>
<p><span class="math display">\[
K_{h}(x^*, \bm{x}_i) = D \left( \frac{ \left || \bm{x}_i - x^* \right || }{ h(x^*) } \right )
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\bm{x}^* \in \mathbb{R}^p\)</span> is the specific point for which we are calculating an estimate</li>
<li><span class="math inline">\(\bm{x}_i \in \mathbb{R}^p\)</span> are the other points in the space</li>
<li><span class="math inline">\(\left || ... \right ||\)</span> is a Norm, be that Euclidean or otherwise</li>
<li><span class="math inline">\(h(\bm{x}^*)\)</span> determines the radius of the Kernel, or equivalently the weighting of a given comparison</li>
<li><span class="math inline">\(D(...)\)</span> is a real-valued function</li>
</ul>
<p>Several Kernels fitting this mould can be seen <a href="https://en.wikipedia.org/wiki/Kernel_(statistics)">here</a>.</p>
<p>Given a Kernel, we can create a continuous function that connects a set of points, i.e. <em>smooths</em> the data. In order to do this, we construct a local weighted average across a dataset. The form this average takes is a generalisation of the traditional weighted arithmetic mean,</p>
<p><span class="math display">\[
\bar{x} = \frac{\sum_{i = 1}^n w_i x_i}{\sum_{i = 1}^n w_i}
\]</span></p>
<p>which, given <span class="math inline">\(n\)</span> observations <span class="math inline">\(x_i\)</span> and corresponding weights <span class="math inline">\(w_i\)</span>, constructs a global average of a given set of points.</p>
<p>The generalisation, named after Èlizbar Nadaraya and Geoffrey Watson, replaces the weights <span class="math inline">\(w_i\)</span> with the result of the Kernel function producing the following form:</p>
<p><span class="math display">\[
\hat{Y} (x^*) = \frac{ \sum_{i=1}^n K_{h}(x^*, \bm{x}_i) Y(\bm{x}_i) }{ \sum_{i=1}^n K_{h}(x^*, \bm{x}_i) }
\]</span></p>
<p>Here,</p>
<ul>
<li><span class="math inline">\(Y(\bm{x}_i)\)</span> returns the magnitude of the sample at <span class="math inline">\(\bm{x}_i\)</span></li>
<li>and <span class="math inline">\(n\)</span> is the number of samples available to build an estimate from</li>
</ul>
<p><img src="../img/2023/KernelSmoothingIllustration.png" id="imgexpandtoborder" class="img" alt="KernelSmoothingIllustration.png" /></p>
<p>For example, given a Kernel function that weights an estimate via the Euclidean distance to its surrounding points, we expect something like the <span class="math inline">\(+\)</span> for the point <span class="math inline">\(\bm{x}^*\)</span> in the image above, as <span class="math inline">\(d1\)</span> is much smaller than <span class="math inline">\(d2\)</span> and <span class="math inline">\(d3\)</span>. Therefore, the point with the most influence on the estimate is the point directly to the left.</p>
<h3>Gaussian Kernel Smoothing</h3>
<p>We create the Gaussian Kernel from the above more general form by setting <span class="math inline">\(D(u)\)</span> to <span class="math inline">\(\text{exp}( -1/2 u^2)\)</span> and <span class="math inline">\(h(x^*)\)</span> to <span class="math inline">\(\sigma\)</span>. In this case, the correct norm is the <span class="math inline">\(\text{L}_2\)</span> or Euclidean norm.</p>
<p><span class="math display">\[
K_{\text{smoother}}(x^*, x) = \text{exp} \left ( - \frac{1}{2} \left ( \frac{ x - x^* }{\sigma} \right )^2 \right )
\]</span></p>
<p>Using this Kernel function and the Nadaraya-Watson estimator, we can then build an estimate for each point <span class="math inline">\(x^*\)</span>. The weighting contributed by each surrounding point falls off in a bell curve-like manner with increased distance. The red, green and yellow curves in the image below show this relative weighting as the <span class="math inline">\(\sigma\)</span> value of the Kernel is varied.</p>
<p><img src="../img/2023/GaussianSmoothing.gif" id="imgexpandtoborder" class="img" alt="GaussianSmoothing.gif" /></p>
<p>These two equations can be almost directly written as Julia code giving us the following implementation.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode julia"><code class="sourceCode julia"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">Ksmoother</span>(x_star, x, σ) <span class="op">=</span> <span class="cn">ℯ</span><span class="op">^</span>( <span class="op">-</span> <span class="fl">1</span><span class="op">/</span><span class="fl">2</span> <span class="op">*</span> (x_star <span class="op">-</span> xs)<span class="op">^</span><span class="fl">2</span> <span class="op">/</span> σ<span class="op">^</span><span class="fl">2</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">NadarayaWatsonEstimate</span>(x_star, xs, Ys, σ)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> <span class="fu">Ksmoother</span>.(x_star, xs, σ)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sum</span>( W <span class="op">.*</span> Ys ) <span class="op">/</span> <span class="fu">sum</span>( W )</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span></code></pre></div>
<p>Having chosen a value <span class="math inline">\(\sigma\)</span>, our estimate for a specific point <span class="math inline">\(x^*\)</span> can then be found as follows</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode julia"><code class="sourceCode julia"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>σ <span class="op">=</span> <span class="fl">0.75</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Surrounding Points</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> [ <span class="fl">0</span>,    <span class="fl">4.21</span>, <span class="fl">5.79</span>, <span class="fl">7.37</span>, <span class="fl">9.47</span> ]</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> [ <span class="fl">0.03</span>, <span class="fl">0.08</span>, <span class="fl">0.93</span>, <span class="fl">0.91</span>, <span class="fl">0.99</span> ]</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>Yhat <span class="op">=</span> <span class="fu">NadarayaWatsonEstimate</span>(x_star, x, Y, σ)</span></code></pre></div>
<h3>Gaussian Kernel Density Estimate</h3>
<p>The main difference between the smoother we constructed, and a density estimate is that we haven't ensured that the area under the curve integrates to <span class="math inline">\(1\)</span>. Our first step is to add the missing normalising term of the <a href="https://en.wikipedia.org/wiki/Normal_distribution">Gaussian normal density function</a> to our Kernel from before, producing</p>
<p><span class="math display">\[
K_\sigma(x^*, x) = \frac{1}{\sigma \sqrt{2 \pi}} \text{exp} \left ( - \frac{1}{2} \left ( \frac{x^* - x}{\sigma} \right )^2 \right )
\]</span></p>
<p>or in Julia</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode julia"><code class="sourceCode julia"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">K</span>(x_star, x, σ) <span class="op">=</span> <span class="fl">1</span><span class="op">/</span>(σ <span class="op">*</span> <span class="fu">√</span>(<span class="fl">2</span><span class="op">*</span><span class="cn">π</span>)) <span class="op">*</span> <span class="cn">ℯ</span><span class="op">^</span>( <span class="op">-</span> <span class="fl">1</span><span class="op">/</span><span class="fl">2</span> <span class="op">*</span> (x_star <span class="op">-</span> x)<span class="op">^</span><span class="fl">2</span> <span class="op">/</span> σ<span class="op">^</span><span class="fl">2</span>)</span></code></pre></div>
<p>Instead of the mean <span class="math inline">\(\mu\)</span> typical of the definition, we have one of the samples, <span class="math inline">\(x\)</span>. In fact, to construct an estimate of our density, we build a mixture from <span class="math inline">\(n\)</span> Gaussian distributions, with the mean of each being one of the samples <span class="math inline">\(x_i\)</span>. Observing this fact then leads to the equation for our probability density estimate.</p>
<p><span class="math display">\[
\hat{f}(x | \sigma, \bm{x}) = \frac{1}{n} \sum_i^n K_\sigma(x, \bm{x}_i)
\]</span></p>
<p>In Julia we can write this as</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode julia"><code class="sourceCode julia"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">f</span>(x, σ, xs) <span class="op">=</span> <span class="fl">1</span><span class="op">/</span><span class="fu">length</span>(xs) <span class="op">*</span> <span class="fu">sum</span>(<span class="fu">K</span>.(x, xs, σ))</span></code></pre></div>
<p>We can similarly construct the cumulative density function estimate by taking the average of the cumulative density of each of the Gaussian distributions. The cumulative density of a single Gaussian, where <span class="math inline">\(\text{erf}\)</span> is the error function, is as follows.</p>
<p><span class="math display">\[
\Phi(x | \mu, \sigma) = \frac{1}{2} \left [ 1 + \text{erf} \left ( \frac{x - \mu}{\sigma \sqrt{2}} \right) \right ]
\]</span></p>
<p>As in the case of the probability density function, our estimate is created by averaging each of the cumulative density functions.</p>
<p><span class="math display">\[
\hat{\text{cdf}} (x| \sigma, \bm{x}) = \frac{1}{n} \sum_i^n \Phi(x | \bm{x}_i, \sigma)
\]</span></p>
<p>By making use of the <code>SpecialFunctions</code> package, we implement these two equations in Julia.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode julia"><code class="sourceCode julia"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">SpecialFunctions</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="fu">Φ</span>(x, μ, σ) <span class="op">=</span> <span class="fl">1</span><span class="op">/</span><span class="fl">2</span> <span class="op">*</span> (<span class="fl">1</span> <span class="op">+</span> <span class="fu">erf</span>((x <span class="op">-</span> μ) <span class="op">/</span> (σ <span class="op">*</span> √<span class="fl">2</span>)))</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="fu">cdf</span>(x, σ, xs) <span class="op">=</span> <span class="fl">1</span><span class="op">/</span><span class="fu">length</span>(xs) <span class="op">*</span> <span class="fu">sum</span>(<span class="fu">Φ</span>.(x, xs, σ))</span></code></pre></div>
<p>We can test these functions by generating samples from a known Gaussian distribution to see whether the estimates match.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode julia"><code class="sourceCode julia"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Distributions</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>μ <span class="op">=</span> <span class="fl">4</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>σ <span class="op">=</span> <span class="fl">2</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>true_distribution <span class="op">=</span> <span class="fu">Normal</span>(μ, σ)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> <span class="fu">rand</span>(true_distribution, <span class="fl">50</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>pdf_estimate <span class="op">=</span> f</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="pp">@show</span> <span class="fu">pdf</span>(true_distribution, <span class="fl">2</span>), <span class="fu">pdf_estimate</span>(<span class="fl">2</span>, σ, samples)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co"># (0.121, 0.115)</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="pp">@show</span> <span class="fu">pdf</span>(true_distribution, <span class="fl">3</span>), <span class="fu">pdf_estimate</span>(<span class="fl">3</span>, σ, samples)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co"># (0.176, 0.141)</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="pp">@show</span> <span class="fu">pdf</span>(true_distribution, <span class="fl">5</span>), <span class="fu">pdf_estimate</span>(<span class="fl">5</span>, σ, samples)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co"># (0.176, 0.137)</span></span></code></pre></div>
<p><img src="../img/2023/GaussianKernelDensity.gif" id="imgexpandtoborder" class="img" alt="GaussianKernelDensity.gif" /></p>
<p>What becomes apparent when plotting the estimate and actual distributions is that the estimate does not match exactly even when the true variance is known and there are several samples. This is because the estimate is not a single density but an average of many.</p>
<h3>Estimating Bandwidth</h3>
<p>The discrepancy we have just seen suggests we need an alternative approach for determining our estimator's <span class="math inline">\(\sigma\)</span> value. In the more general case, where we are not just considering the Gaussian kernel, this weighting is known as the bandwidth. It corresponds to the <span class="math inline">\(h(x^*)\)</span> in our original, more general form above. There are two types of approaches for determining the bandwidth, <em>cross-validation</em> and <em>rule-of-thumb</em>. We will focus on the second here.</p>
<p>Many of the <em>rule-of-thumb</em> approaches start by attempting to minimise the Mean Integrated Square Error (MISE) but simplify its calculation by introducing estimates and simplifications. Given a true probability density function <span class="math inline">\(f\)</span>, and a kernel estimate <span class="math inline">\(\hat{f}\)</span>, the MISE is as follows.</p>
<p><span class="math display">\[\begin{aligned}
\text{MISE}(\hat{f}) &amp;= \int \text{E} \left[ \left( \hat{f}(x) - f(x) \right)^2 \right ] \text{d}x \\
                     &amp;= \int \left( \text{E} \left [ \hat{f}(x) \right ] - f(x) \right )^2 \text{d}x + \int \text{var} \hat{f}(x) \text{d}x
\end{aligned}
\]</span></p>
<p>This can be understood as the sum of the <strong>integrated square bias</strong> and <strong>integrated variance</strong>. During this part of the derivation, we are not restricting ourselves exclusively to the Gaussian Kernel, so we continue with a slightly more general form for our estimate.</p>
<p><span class="math display">\[
\hat{f}(x) = \frac{1}{n} \sum_{i}^n \frac{K \left( \left ( x - y \right ) / h \right )}{h}
\]</span></p>
<p><span class="sidenote-wrapper"><label for="sn-0" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-0" class="margin-toggle" /><span class="sidenote">To fit this form of <span class="math inline">\(\hat{f}\)</span> our Gaussian Kernel is defined
<span class="math display">\[
K(u) = \frac{1}{\sqrt{2\pi}} \text{exp} \left ( - \frac{1}{2} u^2 \right )
\]</span>
This results in the same form as we had above
<span class="math display">\[
\frac{K \left( \left ( x - y \right ) / \sigma \right )}{\sigma} = \frac{1}{\sigma \sqrt{2\pi}} \text{exp} \left ( - \frac{1}{2} \left( \frac{x -y}{\sigma} \right )^2 \right ) = K_\sigma(x, y)
\]</span><br />
<br />
</span></span></p>
<p>As a reminder, we require that our Kernel is <em>symmetric</em>, <em>non-negative</em> and as we use it for density estimation, it should <em>integrate to <span class="math inline">\(1\)</span></em>.</p>
<p>To simplify the <strong>bias</strong> term we make use of the variable substitution <span class="math inline">\(y = x - hu\)</span>, <span class="math inline">\(\text{d}y = h \text{d}u\)</span> and the Taylor expansion</p>
<p><span class="math display">\[
f(x - hu) = f(x) - h u f'(x) + \frac{1}{2}h^2 u^2 f''(x) + ...
\]</span></p>
<p>to get</p>
<p><span class="math display">\[\begin{aligned}
\text{bias} &amp;= \text{E} \left [ \hat{f}(x) \right ] - f(x) \\
                 &amp;= \int \frac{K \left( (x-y) / h \right )}{h} f(y) \text{d}y - f(x) \\
                 &amp;= \int K(u) f(x - hu) \text{d}u - f(x) \\
                 &amp;= \int K(u) \left ( f(x - hu) - f(x) \right ) \text{d}u \\
                 &amp;= -h f'(x) \int u K(u) du + \frac{1}{2} h^2 f''(x) \int u^2 K(u)du + ...
\end{aligned}
\]</span></p>
<p>Due to the required symmetric property, we also have the equality</p>
<p><span class="math display">\[
\int_0^\infty u K(u) \text{d}u + \int_{-\infty}^0 u K(u) \text{d}u = 0 = \int u K(u) \text{d}u
\]</span></p>
<p>as integrating either side of <span class="math inline">\(0\)</span> results in the same absolute area, only the second side will be negative due to <span class="math inline">\(u\)</span>. Consequently, both sides of the integral cancel each other out.</p>
<p>Using this equality, we further simplify the <strong>bias</strong> term to</p>
<p><span class="math display">\[
\text{bias} = \frac{1}{2} h^2 f''(x) \int u^2 K(u)du + ...
\]</span></p>
<p>The term <span class="math inline">\(\int u^2 K(u)du\)</span> is the variance of the Kernel. We replacing it with the variable <span class="math inline">\(k\)</span> for convenience. Now squaring and integrating the result nets us a second-degree approximation of the <strong>integrated square bias</strong></p>
<p><span class="math display">\[
\frac{1}{4} h^4 k^2 \int f''(x)^2 dx
\]</span></p>
<p>Recalling that <strong>variance</strong> can be expressed</p>
<p><span class="math display">\[
\text{var} \hat{f}(x) = \text{E} \left [ (X - \mu)^2 \right ] = \text{E} \left[ X^2 \right ] - \text{E} \left[ X \right ]^2
\]</span></p>
<p>we can use the same process on the variance term to simplify the remaining MISE term</p>
<p><span class="math display">\[\begin{aligned}
\text{variance} &amp;=\frac{1}{n} \int \frac{K \left ( \left ( x - y \right ) / h \right )^2}{h^2} f(y) \text{d}y - \frac{\left ( f(x)  + \text{bias} \right )^2}{n} \\
                        &amp;= \frac{1}{n h} \int f(x - h u) K (u)^2 \text{d}u - \frac{1}{n} \left ( f(x)  + \text{bias} \right )^2 \\
                        &amp;= \frac{1}{n h} \int \left( f(x) - htf'(x) + ... \right ) K (u)^2 \text{d}u - \frac{1}{n} \left ( f(x)  + \text{bias} \right )^2
\end{aligned}
\]</span></p>
<p>Typically, this is further simplified by considering only the case where the number of samples <span class="math inline">\(n\)</span> is large and our bandwidth <span class="math inline">\(h\)</span> is quite small. Under this assumption, the additional Taylor series terms approach zero relative to the initial <span class="math inline">\(f(x)\)</span> term and the <span class="math inline">\(\frac{1}{n} \left ( f(x) + \text{bias} \right )^2\)</span> approaches <span class="math inline">\(0\)</span>. Our <strong>variance</strong> is then</p>
<p><span class="math display">\[
\text{variance} \approx \frac{f(x)}{n h} \int K (u)^2 \text{d}u
\]</span></p>
<p>Raising this term to the power of two and integrating over the <span class="math inline">\(f(x)\)</span> term, which being a probability distribution integrates to one, gives us our <strong>integrated variance</strong> estimate</p>
<p><span class="math display">\[
\int \text{variance } \text{d}x \approx \frac{1}{nh} \int K(u)^2 \text{d}u
\]</span></p>
<p>Together these terms build the Asymptotic Mean Integrated Square Error (AMISE) approximation, used in many bandwidth <em>rule-of-thumb</em> approaches.</p>
<p><span class="math display">\[
\text{AMISE}(h) = \frac{1}{nh} \int K(u)^2 \text{d}u + \frac{1}{4} h^4 k^2 \int f''(x)^2 k dx
\]</span></p>
<p>In the literature this is often shorted to <span class="math inline">\(\frac{R(K)}{nh} + \frac{h^4 k^2 R(f'')}{4}\)</span>, where <span class="math inline">\(R(g) = \int g(x)^2 \text{d}x\)</span>.</p>
<p>We can then find an optimal bandwidth <span class="math inline">\(h\)</span> by deriving our approximation with respect to <span class="math inline">\(h\)</span> and equating it with <span class="math inline">\(0\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
\frac{\text{d}}{\text{d}h} \text{AMISE}(h) &amp; = \frac{\text{d}}{\text{d}h} \frac{R(K)}{nh} + \frac{\text{d}}{\text{d}h} \frac{h^4 k^2 R(f'')}{4} \\
0 &amp;= - \frac{R(K)}{nh^2} + \frac{4 h^3 k^2 R(f'')}{4} \\
h^3 k^2 R(f'') &amp;= \frac{R(K)}{nh^2} \\
h^5 &amp;= \frac{R(K)}{k^2 R(f'')n} \\
h &amp;= \left ( \frac{R(K)}{k^2 R(f'')n} \right ) ^{1/5}
\end{aligned}
\]</span></p>
<p>A more concise derivation of this approximation can be found in both <span class="citation" data-cites="SilvermanDensityEstimationForStatistics">(Silverman 1998, pages 36-40)</span> and <span class="citation" data-cites="WandJonesKernelSmoothing">(Wand 1994, pages 19-22)</span>.</p>
<h3>Optimal Bandwidth For Univariate Gaussian</h3>
<p>Hoping to improve our Julia implementation above, we follow Silverman's treatment in <span class="citation" data-cites="SilvermanDensityEstimationForStatistics">(Silverman 1998, pages 45-48)</span> and use this approximation to estimate the optimal <span class="math inline">\(h\)</span> in the case of a Gaussian Kernel while under the assumption that the true density is a Gaussian distribution.</p>
<p>Starting with the <span class="math inline">\(R(K)\)</span> term, we integrate over the square of our Gaussian Kernel.</p>
<p><span class="math display">\[\begin{aligned}
R(K) &amp;= \int \left ( \frac{1}{\sqrt{2\pi}} \text{exp} \left ( - \frac{1}{2} x^2 \right ) \right)^2 \text{d}x \\
        &amp;= \frac{1}{2 \pi} \int \left ( \text{exp} \left ( - \frac{1}{2} x^2 \right ) \right)^2 \text{d}x \\
        &amp;= \frac{\sqrt{\pi}}{2 \pi} \\
        &amp;= \frac{1}{2\sqrt{\pi}}
\end{aligned}
\]</span></p>
<p>Next, using our assumption that the true distribution is Gaussian, i.e. that <span class="math inline">\(f = \frac{1}{\sigma \sqrt{2 \pi}} \text{exp} \left( - \frac{1}{2} \frac{x^2}{\sigma^2} \right )\)</span>, we can solve for the <span class="math inline">\(R(f'')\)</span> term.</p>
<p><span class="math display">\[\begin{aligned}
R(f'') &amp;= \int \left( \frac{\text{d}^2}{\text{d}x} \frac{1}{\sigma \sqrt{2 \pi}} \text{exp} \left( - \frac{1}{2} \frac{x^2}{\sigma^2} \right ) \right )^2 \text{d} x \\
       &amp;= \frac{1}{\sigma^{10} 2 \pi}  \int \left( (x^2 - \sigma^2) \text{exp} \left( - \frac{1}{2} \frac{x^2}{\sigma^2} \right ) \right )^2 \text{d} x \\
       &amp;= \frac{1}{\sigma^{10} 2 \pi} \left( \frac{3 \sqrt{\pi} \sigma^5 }{4} \right ) \\
       &amp;= \frac{3}{8 \sigma^{5} \sqrt{\pi}}
\end{aligned}
\]</span></p>
<p>Remembering that <span class="math inline">\(k\)</span> is the variance of our Kernel, in this case, equal to <span class="math inline">\(1\)</span> as our Kernel is a standard Gaussian, our approximate optimal bandwidth is</p>
<p><span class="math display">\[\begin{aligned}
h_{gaussian} &amp;= (2 \sqrt{\pi})^{-1/5} \left ( \frac{3}{8 \sigma^{5} \sqrt{\pi}} \right )^{-1/5} (1^2 n)^{-1/5} \\
             &amp;= \left ( \frac{8}{6} \right )^{1/5} \sigma n^{-1/5} \\
             &amp;\approx 1.06 \sigma n^{-1/5}
\end{aligned}
\]</span></p>
<p>We can now implement this approximation in Julia, equating <span class="math inline">\(\sigma^2\)</span> with an unbiased estimate of the variance of our samples<span class="sidenote-wrapper"><label for="sn-1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-1" class="margin-toggle" /><span class="sidenote">The unbiased variance can be calculated as follows
<span class="math display">\[
\begin{aligned}
\bar{x} &amp;= \frac{\sum_i^n x_i}{n} \\
s^2 &amp;= \frac{n^{-1} \sum_i^n (x_i - \bar{x})^2}{1 - n^{-1}}
\end{aligned}
\]</span>
where <span class="math inline">\(1 - n^{-1}\)</span> is the bias correction term<br />
<br />
</span></span>.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode julia"><code class="sourceCode julia"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">h_est_gaussian</span>(samples)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    μ <span class="op">=</span> <span class="fu">sum</span>(samples) <span class="op">/</span> <span class="fu">length</span>(samples)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    unbiased_variance <span class="op">=</span> <span class="fu">sum</span>((samples <span class="op">.-</span> μ) <span class="op">.^</span> <span class="fl">2</span>) <span class="op">/</span> ( <span class="fu">length</span>(samples) <span class="op">-</span> <span class="fl">1</span> )</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="fl">1.06</span> <span class="op">*</span> <span class="fu">√</span>(unbiased_variance) <span class="op">*</span> <span class="fu">length</span>(samples) <span class="op">^</span> (<span class="op">-</span><span class="fl">1</span> <span class="op">/</span> <span class="fl">5</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span></code></pre></div>
<p>Comparing this new estimate with our previous estimate and the true probability, we see an improvement in the predicted values. This is because the bandwidth of the Kernels is smaller, causing less smoothing and allowing for a larger range of predicted values.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode julia"><code class="sourceCode julia"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>μ <span class="op">=</span> <span class="fl">4</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>σ <span class="op">=</span> <span class="fl">2</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>true_distribution <span class="op">=</span> <span class="fu">Normal</span>(μ, σ)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> <span class="fu">rand</span>(true_distribution, <span class="fl">50</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>pdf_estimate <span class="op">=</span> f</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="fu">pdf_estimate_gaus_opt_h</span>(x, samples) <span class="op">=</span> <span class="fu">f</span>(x, <span class="fu">h_est_gaussian</span>(samples), samples)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="pp">@show</span> <span class="fu">pdf</span>(true_distribution, <span class="fl">2</span>),</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>      <span class="fu">pdf_estimate</span>(<span class="fl">2</span>, σ, samples),</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>      <span class="fu">pdf_estimate_gaus_opt_h</span>(<span class="fl">2</span>, samples)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="co"># (0.121, 0.115, 0.111)</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="pp">@show</span> <span class="fu">pdf</span>(true_distribution, <span class="fl">3</span>),</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>      <span class="fu">pdf_estimate</span>(<span class="fl">3</span>, σ, samples),</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>      <span class="fu">pdf_estimate_gaus_opt_h</span>(<span class="fl">3</span>, samples)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="co"># (0.176, 0.141, 0.185)</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="pp">@show</span> <span class="fu">pdf</span>(true_distribution, <span class="fl">5</span>),</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>      <span class="fu">pdf_estimate</span>(<span class="fl">5</span>, σ, samples),</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>      <span class="fu">pdf_estimate_gaus_opt_h</span>(<span class="fl">5</span>, samples)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a><span class="co"># (0.176, 0.137, 0.173)</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a><span class="pp">@show</span> <span class="fu">h_est_gaussian</span>(samples)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.907</span></span></code></pre></div>
<p><img src="../img/2023/EstimateComparison.svg" id="imgexpandtoborder" class="img" alt="EstimateComparison.svg" /></p>
<p>In the same way, we could formulate an approximately optimal bandwidth for all potential density functions <span class="math inline">\(f\)</span>, assuming we can find <span class="math inline">\(f\)</span>'s second derivative. We do not, however, in general, know the true distribution <span class="math inline">\(f\)</span>; our motivation to use Kernel density estimation in place of directly fitting a distribution. The true distribution may be multimodal or skewed heavily, causing over-smoothing. Silverman's now ubiquitous <em>Rule of Thumb</em> takes the previously derived optimal <span class="math inline">\(h_{gaussian}\)</span> and tries to account for some of these possibilities. His first suggestion is to use the interquartile range (IQR) when it is smaller than the standard deviation of the data. Therefore, in the case of skewed distributions, fewer samples will influence the estimate at each point, reducing overall smoothing and allowing for a more locally defined estimate. In other words, we reduce the influence of our choosing a Gaussian Kernel when it seems less likely that the data follows a Gaussian distribution. Following similar reasoning, he also suggests reducing the <span class="math inline">\(1.06\)</span> term to <span class="math inline">\(0.9\)</span> with experiments showing improvement when <span class="math inline">\(f\)</span> is skewed or bimodal with marginal degradation in the case of a Gaussian. The result of these modifications is Silverman's <em>Rule of Thumb</em></p>
<p><span class="math display">\[
h = 0.9 \text{ min} \left ( \sigma , \frac{IQR}{1.34}  \right ) n^{-1/5}
\]</span></p>
<p><span class="citation" data-cites="SilvermanDensityEstimationForStatistics">(Silverman 1998, page 48)</span> <span class="sidenote-wrapper"><label for="sn-2" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-2" class="margin-toggle" /><span class="sidenote">In the case of a standard Gaussian distribution, the IQR is approximately <span class="math inline">\(1.34\sigma\)</span><br />
<br />
</span></span></p>
<p>In Julia, we can write this as follows, again employing an unbiased estimate of our data's variance</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode julia"><code class="sourceCode julia"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">StatsBase</span>: quantile, Weights</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">h_est_silverman</span>(samples)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    μ <span class="op">=</span> <span class="fu">sum</span>(samples) <span class="op">/</span> <span class="fu">length</span>(samples)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    unbiased_variance <span class="op">=</span> <span class="fu">sum</span>((samples <span class="op">.-</span> μ) <span class="op">.^</span> <span class="fl">2</span>) <span class="op">/</span> ( <span class="fu">length</span>(samples) <span class="op">-</span> <span class="fl">1</span> )</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    (q1, q3) <span class="op">=</span> <span class="fu">quantile</span>(samples, [<span class="fl">0.25</span>, <span class="fl">0.75</span>])</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    IQR <span class="op">=</span> q3 <span class="op">-</span> q1</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    <span class="fl">0.9</span> <span class="op">*</span> <span class="fu">min</span>( <span class="fu">√</span>(unbiased_variance) , IQR <span class="op">/</span> <span class="fl">1.34</span>) <span class="op">*</span> <span class="fu">length</span>(samples) <span class="op">^</span> (<span class="op">-</span><span class="fl">1</span> <span class="op">/</span> <span class="fl">5</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span></code></pre></div>
<p><img src="../img/2023/EstimateComparison2.png" id="imgexpandtoborder" class="img" alt="EstimateComparison2.png" /></p>
<p>Instead of making assumptions about the true distribution <span class="math inline">\(f\)</span>, we can opt for a nonparametric approach. One such method is explored in <span class="citation" data-cites="SheaterJonesReliableDataBasedBandwidth">(Sheather and Jones 1991)</span>. In the paper, the authors also proceed from the AMISE and, through a somewhat more involved derivation, reach the following result (equation 12 in the paper), similar to our above approximately optimal <span class="math inline">\(h\)</span> form, but with the <span class="math inline">\(R(f'')\)</span> term where we made assumptions about the true density modified.</p>
<p><span class="math display">\[
\left ( \frac{R(K)}{n k^2 \hat{S}_D(\hat{\alpha}_2(h))} \right)^{\frac{1}{5}} - h = 0
\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{aligned}
\hat{S}_D(\alpha) &amp;= \frac{1}{\alpha^5 n(n-1)} \sum_i^n \sum_j^n \phi^{\text{iv}} \left ( \frac{X_i-X_j}{\alpha} \right ) \\
\hat{T}_D(b) &amp;= - \frac{1}{b^7 n(n-1)} \sum_i^n \sum_j^n \phi^{\text{vi}} \left( \frac{X_i-X_j}{b} \right ) \\
\hat{\alpha}_2(h) &amp;= 1.357 \left ( \frac{\hat{S}_D(\alpha)}{\hat{T}_D(b)} \right )^{1/7} h^{5/7} \\
a &amp;= \frac{0.920 \times \text{IQR}}{n^{1/7}} \\
b &amp;= \frac{0.912 \times IQR}{n^{1/9}}
\end{aligned}
\]</span></p>
<p>In contrast to Silverman's <em>Rule of Thumb</em>, this approach additionally considers an estimate of the third derivative of our true density <span class="math inline">\(f\)</span> and requires we calculate the fourth (<span class="math inline">\(\phi^{\text{iv}}\)</span>) and sixth (<span class="math inline">\(\phi^{\text{vi}}\)</span>) derivative of our chosen Kernel function.</p>
<p>Continuing our example with the Gaussian Kernel, these derivatives would be</p>
<p><span class="math display">\[
\begin{aligned}
\phi^{\text{iv}} &amp;= \frac{\text{d}^4}{\text{d}u^4} K(u) = \frac{u^4 - 6u^2 + 3}{\sqrt{2 \pi}} \text{exp} \left ( - \frac{1}{2} u^2 \right ) \\
\phi^{\text{vi}} &amp;= \frac{\text{d}^6}{\text{d}u^6} K(u) = \frac{u^6 - 15u^4 + 45u^2 - 15}{\sqrt{2 \pi}} \text{exp} \left ( - \frac{1}{2} u^2 \right ) \\
\end{aligned}
\]</span></p>
<p>It is now necessary for us to search for a bandwidth <span class="math inline">\(h\)</span> that minimises the equation. Instead of implementing a solver ourselves, we can use the Julia <code>Optim</code> package.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode julia"><code class="sourceCode julia"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Optim</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">h_est_sheatherjones</span>(samples)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="fu">length</span>(samples)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 4th and 6th derivatives of the Kernel</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ϕ is the standard normal density</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ϕIV</span>(u) <span class="op">=</span> (u<span class="op">^</span><span class="fl">4</span> <span class="op">-</span> <span class="fl">6</span><span class="op">*</span>u<span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="fl">3</span>) <span class="op">*</span> <span class="fl">1</span><span class="op">/</span><span class="fu">√</span>(<span class="fl">2</span>π) <span class="op">*</span> <span class="cn">ℯ</span><span class="op">^</span>(<span class="op">-</span><span class="fl">1</span><span class="op">/</span><span class="fl">2</span> <span class="op">*</span> u<span class="op">^</span><span class="fl">2</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ϕVI</span>(u) <span class="op">=</span> (u<span class="op">^</span><span class="fl">6</span> <span class="op">-</span> <span class="fl">15</span><span class="op">*</span>u<span class="op">^</span><span class="fl">4</span> <span class="op">+</span> <span class="fl">45</span><span class="op">*</span>u<span class="op">^</span><span class="fl">2</span> <span class="op">-</span> <span class="fl">15</span>) <span class="op">*</span> <span class="fl">1</span><span class="op">/</span><span class="fu">√</span>(<span class="fl">2</span>π) <span class="op">*</span> <span class="cn">ℯ</span><span class="op">^</span>(<span class="op">-</span><span class="fl">1</span><span class="op">/</span><span class="fl">2</span> <span class="op">*</span> u<span class="op">^</span><span class="fl">2</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    (q1, q3) <span class="op">=</span> <span class="fu">quantile</span>(samples, [<span class="fl">0.25</span>, <span class="fl">0.75</span>])</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    IQR <span class="op">=</span> q3 <span class="op">-</span> q1</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> <span class="fl">0.92</span> <span class="op">*</span> IQR <span class="op">*</span> n <span class="op">^</span> (<span class="op">-</span><span class="fl">1</span> <span class="op">/</span> <span class="fl">7</span>)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> <span class="fl">0.912</span> <span class="op">*</span> IQR <span class="op">*</span> n <span class="op">^</span> (<span class="op">-</span><span class="fl">1</span> <span class="op">/</span> <span class="fl">9</span>)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Only calculate half of the distances as each side is symmetric</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># storing in a bottom triangular matrix</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    samplediffs <span class="op">=</span> <span class="fu">zeros</span>(<span class="fu">Int</span>(<span class="fu">n*</span>(n<span class="op">-</span><span class="fl">1</span>)<span class="op">/</span><span class="fl">2</span>))</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> <span class="fl">1</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> r <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span>n, c <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span>r<span class="op">-</span><span class="fl">1</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (r-1) * (r-2) ÷ 2 + c</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>        samplediffs[idx] <span class="op">=</span> samples[r] <span class="op">-</span> samples[c]</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">+=</span> <span class="fl">1</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Multiplied by 2 as only have half of the distance matrix</span></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>    TDb <span class="op">=</span> <span class="op">-</span><span class="fl">2</span> <span class="op">/</span> (n <span class="op">*</span> (n<span class="op">-</span><span class="fl">1</span>) <span class="op">*</span> b<span class="op">^</span><span class="fl">7</span>) <span class="op">*</span> <span class="fu">sum</span>( <span class="fu">ϕVI</span>.(samplediffs <span class="op">./</span> b) )</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>    SDα <span class="op">=</span>  <span class="fl">2</span> <span class="op">/</span> (n <span class="op">*</span> (n<span class="op">-</span><span class="fl">1</span>) <span class="op">*</span> a<span class="op">^</span><span class="fl">5</span>) <span class="op">*</span> <span class="fu">sum</span>( <span class="fu">ϕIV</span>.(samplediffs <span class="op">./</span> a) )</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">function</span> <span class="fu">absolute_error</span>(h)</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> h[<span class="fl">1</span>]</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>        α<span class="fl">2</span>h <span class="op">=</span> <span class="fl">1.357</span> <span class="op">*</span> ( SDα <span class="op">/</span> TDb ) <span class="op">^</span> (<span class="fl">1</span><span class="op">/</span><span class="fl">7</span>) <span class="op">*</span> h <span class="op">^</span> (<span class="fl">5</span><span class="op">/</span><span class="fl">7</span>)</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>        SDα<span class="fl">2</span> <span class="op">=</span> <span class="fl">2</span> <span class="op">/</span> (n <span class="op">*</span> (n<span class="op">-</span><span class="fl">1</span>) <span class="op">*</span> α<span class="fl">2</span>h<span class="op">^</span><span class="fl">5</span>) <span class="op">*</span> <span class="fu">sum</span>( <span class="fu">ϕIV</span>.(samplediffs <span class="op">./</span> α<span class="fl">2</span>h) )</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Want == 0</span></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>        <span class="fu">abs</span>( (<span class="fl">1</span> <span class="op">/</span> (<span class="fl">2</span> <span class="op">*</span> √π <span class="op">*</span> SDα<span class="fl">2</span> <span class="op">*</span> n)) <span class="op">^</span> (<span class="fl">1</span><span class="op">/</span><span class="fl">5</span>) <span class="op">-</span> h )</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>    <span class="kw">end</span></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> <span class="fu">optimize</span>(</span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>        absolute_error,</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>        <span class="fl">0</span>,                               <span class="co"># Search Min Bound</span></span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>        <span class="fu">h_est_silverman</span>(samples) <span class="op">*</span> <span class="fl">1000</span>, <span class="co"># Search Max Bound</span></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>        <span class="fu">Brent</span>()</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>    <span class="pp">@assert</span> Optim.<span class="fu">converged</span>(result)</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> Optim.<span class="fu">minimizer</span>(result)[<span class="fl">1</span>]</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span></code></pre></div>
<p><img src="../img/2023/EstimateComparison3.png" id="imgexpandtoborder" class="img" alt="EstimateComparison3.png" /></p>
<h3>Working With Weighted Samples</h3>
<p>Sometimes we want the influence of each sample on the result to differ. For example, when constructing a customer spending model, we could weight each transaction with the customer's transaction frequency. This would reduce the bias of high-volume customers on the model.</p>
<p>There are two types of weights; integer count weights and fractional proportions (our example is the second). We can still create an unbiased estimate via Bessel correction in the first case, as in the implementations above. In the proportion case, however, we have no information about the total size of our sample.<span class="sidenote-wrapper"><label for="sn-3" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-3" class="margin-toggle" /><span class="sidenote">This topic has provoked discussion on the statics StachExchange. See <a href="https://stats.stackexchange.com/questions/47325/bias-correction-in-weighted-variance#">Bias correction in weighted variance</a>, <a href="https://stats.stackexchange.com/questions/51442/weighted-variance-one-more-time/61285#">Weighted Variance, one more time</a> and <a href="https://stats.stackexchange.com/questions/61225/correct-equation-for-weighted-unbiased-sample-covariance/61298#61298">Correct equation for weighted unbiased sample covariance</a>.<br />
<br />
</span></span> We may, therefore, choose to forego applying the correction or instead attempt to approximate the number of samples. For example, via Kish's <em>Rule of Thumb</em>.</p>
<p><span class="math display">\[
n_{\text{eff}} = \frac{\left ( \sum_i^n w_i \right ) ^2}{\sum_i^n w_i^2}
\]</span></p>
<p><span class="sidenote-wrapper"><label for="sn-4" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-4" class="margin-toggle" /><span class="sidenote">When all weights are equal
<span class="math display">\[
n_{\text{eff}} = \frac{\left ( \sum_i^n \frac{1}{n} \right ) ^2}{\sum_i^n \frac{1}{n^2}} = \frac{1^2}{\frac{n}{n^2}} = n
\]</span><br />
<br />
</span></span></p>
<p>If we have integer-type weights or choose to use Kish's <em>Rule of Thumb</em>, we end up with an analogous term to the Bessel correction for individual equally weighted samples. The <span class="math inline">\(n\)</span> in the <span class="math inline">\(1 - n^{-1}\)</span> correction term is effectively replaced by <span class="math inline">\(n_{\text{eff}}\)</span> giving us the following form for an unbiased weighted variance. The <a href="https://en.wikipedia.org/wiki/Weighted_arithmetic_mean#Weighted_sample_variance">Wikipedia page</a> has a full derivation.</p>
<p><span class="math display">\[
\begin{aligned}
\bar{x}_{\text{weighted}} &amp;= \frac{\sum_i^n w_i \times x_i}{\sum_i^n w_i} \\
s_{\text{weighted}} &amp;= \frac{ \left ( \sum_i^n w_i \right )^{-1} \left( \sum_i^n w_i (x_i - \bar{x_i})^2 \right ) }{ 1 - n_{\text{eff}}^{-1} } \\
\end{aligned}
\]</span></p>
<p><span class="sidenote-wrapper"><label for="sn-5" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-5" class="margin-toggle" /><span class="sidenote">If all of the samples have a weight of one, we recover the traditional equation for non-biased variance, as both <span class="math inline">\(\sum_i^n w_i\)</span> and <span class="math inline">\(n_{\text{eff}}\)</span> will equal <span class="math inline">\(n\)</span>.<br />
<br />
</span></span></p>
<p>Using this definition we modify Silverman's <em>Rule of Thumb</em> for weighted samples so that bandwidth estimates are still possible.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode julia"><code class="sourceCode julia"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">StatsBase</span>: quantile, Weights</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">h_est_silverman</span>(samples, ws)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    μ <span class="op">=</span> <span class="fu">sum</span>(ws <span class="op">.*</span> samples) <span class="op">/</span> <span class="fu">sum</span>(ws)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    neff <span class="op">=</span> <span class="fu">sum</span>(ws)<span class="op">^</span><span class="fl">2</span> <span class="op">/</span> <span class="fu">sum</span>(ws <span class="op">.^</span> <span class="fl">2</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    unbiased_variance <span class="op">=</span> <span class="fl">1</span> <span class="op">/</span> <span class="fu">sum</span>(ws) <span class="op">*</span> <span class="fu">sum</span>(ws <span class="op">.*</span> (samples <span class="op">.-</span> μ) <span class="op">.^</span> <span class="fl">2</span>) <span class="op">/</span> ( <span class="fl">1</span> <span class="op">-</span> <span class="fl">1</span> <span class="op">/</span> neff )</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We also calculate a weighted IQR</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    (q1, q3) <span class="op">=</span> <span class="fu">quantile</span>(samples, <span class="fu">Weights</span>(ws), [<span class="fl">0.25</span>, <span class="fl">0.75</span>])</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    IQR <span class="op">=</span> q3 <span class="op">-</span> q1</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span class="fl">0.9</span> <span class="op">*</span> <span class="fu">min</span>( <span class="fu">√</span>(unbiased_variance) , IQR <span class="op">/</span> <span class="fl">1.34</span>) <span class="op">*</span> <span class="fu">length</span>(samples) <span class="op">^</span> (<span class="op">-</span><span class="fl">1</span> <span class="op">/</span> <span class="fl">5</span>)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span></code></pre></div>
<p><span class="sidenote-wrapper"><label for="sn-6" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-6" class="margin-toggle" /><span class="sidenote">The weighted IQR can also make use of Kish's <em>Rule of Thumb</em>. A wonderful outline can be found <a href="https://aakinshin.net/posts/weighted-quantiles/">here</a>.<br />
<br />
</span></span></p>
<p>A more in depth exploration of this modification to Silverman's <em>Rule of Thumb</em> can be found in <span class="citation" data-cites="WangBandwidthWeightedKDE">(Wang and Wang 2007)</span>. Unfortunately, the Sheather-Jones method can't be modified as easily for weighted samples.</p>
<p>To account for the weights in our density estimate, we must similarly modify our estimator definition. We have the following form: analogous to the difference between a mean and a weighted mean.</p>
<p><span class="math display">\[
\hat{f}(x | \sigma, \bm{x}, \bm{w}) = \frac{ \sum_i^n \bm{w}_i K_\sigma(x, \bm{x}_i) }{\sum_i^n \bm{w}_i}
\]</span></p>
<p>In Julia we can write this as</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode julia"><code class="sourceCode julia"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">f</span>(x, σ, xs, ws) <span class="op">=</span> <span class="fu">sum</span>(ws <span class="op">.*</span> <span class="fu">K</span>.(x, xs, σ)) <span class="op">/</span> <span class="fu">sum</span>(ws)</span></code></pre></div>
<hr />
<h3 class="unnumbered">References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-SheaterJonesReliableDataBasedBandwidth" class="csl-entry" role="doc-biblioentry">
Sheather, S. J., and M. C. Jones. 1991. <span>“A reliable data-based bandwidth selection method for kernel density estimation.”</span> <em>Journal of the Royal Statistical Society. Series B (Methodological)</em> 53 (3): 683–690. <a href="http://www.jstor.org/stable/2345597">http://www.jstor.org/stable/2345597</a>.
</div>
<div id="ref-SilvermanDensityEstimationForStatistics" class="csl-entry" role="doc-biblioentry">
Silverman, B. W. 1998. <em>Density estimation for statistics and data analysis</em>. 1st ed. Routledge. <a href="https://doi.org/10.1201/9781315140919">https://doi.org/10.1201/9781315140919</a>.
</div>
<div id="ref-WandJonesKernelSmoothing" class="csl-entry" role="doc-biblioentry">
Wand, &amp; Jones, M. P. 1994. <em>Kernel smoothing</em>. 1st ed. Chapman; Hall/CRC. <a href="https://doi.org/10.1201/b14876">https://doi.org/10.1201/b14876</a>.
</div>
<div id="ref-WangBandwidthWeightedKDE" class="csl-entry" role="doc-biblioentry">
Wang, Bin, and Xiaofeng Wang. 2007. <span>“Bandwidth selection for weighted kernel density estimation.”</span> <em>arXiv preprint arXiv:0709.1616</em>.
</div>
</div>

            </div>
            <div id="footer">
                Site proudly generated by
                <a href="http://jaspervdj.be/hakyll">Hakyll</a>
            </div>
        </div>

        <!-- TODO Includes need to be cleaned up. -->
        <link rel="stylesheet" href="../katex/katex.min.css">
        <script defer src="../katex/katex.min.js"></script>
        <script type="text/javascript" script defer src="../katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
        
        
        
        <!-- TODO Includes need to be cleaned up. -->

    </body>
</html>

@article{BentleyMultidimensionalDivideAndConquer,
  author = {Bentley, Jon Louis},
  title = {Multidimensional Divide-and-Conquer},
  year = {1980},
  issue_date = {April 1980},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {23},
  number = {4},
  issn = {0001-0782},
  url = {https://doi.org/10.1145/358841.358850},
  doi = {10.1145/358841.358850},
  abstract = {Most results in the field of algorithm design are single algorithms that solve single problems. In this paper we discuss multidimensional divide-and-conquer, an algorithmic paradigm that can be instantiated in many different ways to yield a number of algorithms and data structures for multidimensional problems. We use this paradigm to give best-known solutions to such problems as the ECDF, maxima, range searching, closest pair, and all nearest neighbor problems. The contributions of the paper are on two levels. On the first level are the particular algorithms and data structures given by applying the paradigm. On the second level is the more novel contribution of this paper: a detailed study of an algorithmic paradigm that is specific enough to be described precisely yet general enough to solve a wide variety of problems.},
  journal = {Commun. ACM},
  month = {apr},
  pages = {214–229},
  numpages = {16},
  keywords = {empirical cumulative distribution functions, computational geometry, analysis of algorithms, multidimensional searching problems, closest-point problem, maxima problems, data structures, algorithmic paradigms, range searching}
}

@book{BoydVandenbergheConvexOptimization,
  author = {Boyd, Stephen and Vandenberghe, Lieven},
  title = {Convex Optimization},
  year = {2004},
  isbn = {0521833787},
  publisher = {Cambridge University Press},
  address = {USA}
}

@inproceedings{LussRossetDecomposingIsotonicRegression,
  author = {Luss, Ronny and Rosset, Saharon and Shahar, Moni},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
  pages = {},
  publisher = {Curran Associates, Inc.},
  title = {Decomposing Isotonic Regression for Efficiently Solving Large Problems},
  url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/03c6b06952c750899bb03d998e631860-Paper.pdf},
  volume = {23},
  year = {2010}
}

@article{LussRossetEfficientRegularizedIsotonicRegression,
  author = {Luss, Ronny, Rosset, Saharon and Shahar, Moni},
  title = {Efficient regularized isotonic regression with application to gene–gene interaction search},
  volume = {6},
  journal = {The Annals of Applied Statistics},
  number = {1},
  publisher = {Institute of Mathematical Statistics},
  pages = {253 -- 283},
  keywords = {Multivariate isotonic regression, Nonparametric regression, partitioning, regularization path},
  year = {2012},
  doi = {10.1214/11-AOAS504},
  URL = {https://doi.org/10.1214/11-AOAS504}
}

@article{LussRossetGeneralizedIsotonicRegression,
  author = {Ronny Luss and Saharon Rosset},
  title = {Generalized Isotonic Regression},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {23},
  number = {1},
  pages = {192-210},
  year = {2014},
  publisher = {Taylor & Francis},
  doi = {10.1080/10618600.2012.741550},
  URL = {https://doi.org/10.1080/10618600.2012.741550},
  eprint = {https://doi.org/10.1080/10618600.2012.741550}
}

@article{SheaterJonesReliableDataBasedBandwidth,
  ISSN = {00359246},
  URL = {http://www.jstor.org/stable/2345597},
  abstract = {We present a new method for data-based selection of the bandwidth in kernel density estimation which has excellent properties. It improves on a recent procedure of Park and Marron (which itself is a good method) in various ways. First, the new method has superior theoretical performance; second, it also has a computational advantage; third, the new method has reliably good performance for smooth densities in simulations, performance that is second to none in the existing literature. These methods are based on choosing the bandwidth to (approximately) minimize good quality estimates of the mean integrated squared error. The key to the success of the current procedure is the reintroduction of a non-stochastic term which was previously omitted together with use of the bandwidth to reduce bias in estimation without inflating variance.},
  author = {S. J. Sheather and M. C. Jones},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  number = {3},
  pages = {683--690},
  publisher = {[Royal Statistical Society, Wiley]},
  title = {A Reliable Data-Based Bandwidth Selection Method for Kernel Density Estimation},
  urldate = {2023-06-04},
  volume = {53},
  year = {1991}
}

@book{SilvermanDensityEstimationForStatistics,
  title={Density Estimation for Statistics and Data Analysis},
  author={Silverman, B.W.},
  year={1998},
  publisher={Routledge},
  edition={1},
  url={https://doi.org/10.1201/9781315140919},
  pages={34--48}
}

@book{WandJonesKernelSmoothing,
  title={Kernel Smoothing},
  author={Wand, M.P., & Jones, M.C.},
  year={1994},
  publisher={Chapman and Hall/CRC},
  edition={1},
  url={https://doi.org/10.1201/b14876},
  pages={19--23}
}

@article{WangBandwidthWeightedKDE,
  title={Bandwidth selection for weighted kernel density estimation},
  author={Wang, Bin and Wang, Xiaofeng},
  journal={arXiv preprint arXiv:0709.1616},
  year={2007},
  publisher={Citeseer}
}
